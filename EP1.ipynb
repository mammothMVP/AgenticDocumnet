{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dc0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "  host = \"gateway01.ap-southeast-1.prod.aws.tidbcloud.com\",\n",
    "  port = 4000,\n",
    "  user = \"2B3nARpwz82J49u.root\",\n",
    "  password = \"tpc7Iw6jTcsoavz2\",\n",
    "  database = \"TextToVector\",\n",
    "  ssl_ca = \"isrgrootx1 (2).pem\",\n",
    "  ssl_verify_cert = True,\n",
    "  ssl_verify_identity = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edf620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x0000027BB6F823D0>\n"
     ]
    }
   ],
   "source": [
    "print(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d2f228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ee8319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "PDF loaded successfully: scammer-agent.pdf\n"
     ]
    }
   ],
   "source": [
    "local_path = 'scammer-agent.pdf'\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "    print(f'PDF loaded successfully: {local_path}')\n",
    "else:\n",
    "    print('Upload a PDF file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487abe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'scammer-agent.pdf'}, page_content='4 2 0 2\\n\\nt c O 1 2\\n\\n] I\\n\\nA . s c [\\n\\n1 v 0 5 6 5 1 . 0 1 4 2 : v i X r a\\n\\nVoice-Enabled AI Agents can Perform Common Scams\\n\\nRichard Fang UIUC\\n\\nDylan Bowman UIUC\\n\\nDaniel Kang UIUC\\n\\nAbstract\\n\\nRecent advances in multi-modal, highly ca- pable LLMs have enabled voice-enabled AI agents. These agents are enabling new applica- tions, such as voice-enabled autonomous cus- tomer service. However, with all AI capabili- ties, these new capabilities have the potential for dual use.\\n\\nIn this work, we show that voice-enabled AI agents can perform the actions necessary to perform common scams. To do so, we select a list of common scams collected by the govern- ment and construct voice-enabled agents with directions to perform these scams. We con- duct experiments on our voice-enabled agents and show that they can indeed perform the ac- tions necessary to autonomously perform such scams. Our results raise questions around the widespread deployment of voice-enabled AI agents.\\n\\n1\\n\\nIntroduction\\n\\nAI capabilities have advanced rapidly in the past few years. Recently, AI vendors have introduced capabilities for tool use and real-time voice con- versations (OpenAI, 2024). Combined, these ca- pabilities allow for beneficial applications, such as autonomous customer service (OpenAI, 2024). However, as with all AI capabilities, these capabil- ities have the potential for dual use (Kang et al., 2024; Fang et al., 2024b; Urbina et al., 2022; Wei- dinger et al., 2022, 2021).\\n\\nTools\\n\\nStolen funds\\n\\nBank websiteResponse\\n\\nScammer agentVictimGPT-4o\\n\\nFigure 1: Architecture diagram of a voice scammer agent.\\n\\nwork, we focus specifically on the actions needed to perform the scams and do not consider questions of persuading victims.\\n\\nWe conduct a series of experiments, showing that voice-enabled AI agents are highly capable of autonomously performing the actions needed to conduct these common scams. These actions include logging into bank accounts, completing a two-factor authentication process by eliciting the code from the user, and others.\\n\\n2 Common Scams\\n\\nPhone-based scams are incredibly prevalent. Ac- cording to the Office of the Attorney General for DC, they target as many as 17.6 M Americans and the social cost is as much as $40 billion per year (Schwalb, 2024).\\n\\nIn this work, we investigate the question: can voice-enabled AI agents perform the tasks needed to conduct common scams? We answer the ques- tion in the affirmative, showing that voice-enabled AI agents can perform common scams in real-time. To do so, we first identify a list of common scams as collected by the government (Paxton, 2024). From these scams, we designed voice- enabled AI agents with directions to conduct these scams with access to simple tools (Figure 1). In this\\n\\nThese scams typically involve a scammer calling a victim to convince them to take actions or re- veal sensitive information. Based on these actions or information, the scammer then typically steals funds from the victim. We provide a list of com- mon scams in Table 1 as provided by the Attorney General of Texas’ office (Paxton, 2024).\\n\\nPerforming these scams can require complex interactions with websites and feedback from the user. Consider a scam where the scammer steals\\n\\n1\\n\\na victim’s bank credentials and steals money by transferring it out. In order to perform this scam, the scammer must:\\n\\n1. Navigate to the bank website.\\n\\n2. Retrieve the user’s username and password.\\n\\n3. Retrieve the user’s two-factor authentication code.\\n\\n4. Navigate to the transfer page.\\n\\n5. Transfer the money.\\n\\nThe scammer must also react to any errors that may occur (e.g., a misheard password).\\n\\nAs part of the scam, the scammer must also per- suade the victim that the scammer is legitimate. In this work, we do not focus on the persuasion aspect of the scam. We instead focus specifically on the actions needed to perform the scams. However, prior work has shown that LLMs can be highly convincing, potentially even more convincing than people (Salvi et al., 2024).\\n\\n3 Agent Design\\n\\nWe designed a series of agents to perform the ac- tions necessary for common scams. Our agents consist of a base, voice-enabled LLM (GPT-4o), a set of tools that the LLM can use, and scam-specific instructions. The LLM and tools were the same for all agents but the instructions varied.\\n\\nThe AI agents had access to five browser ac- cess tools based on the browser testing framework playwright. These tools are granular browser ac- tions:\\n\\n1. get_html, which gets the HTML of a page.\\n\\n2. navigate, which navigates to a specific URL.\\n\\n3. click_element, which clicks on an element with a CSS selector.\\n\\n4. fill_element, which fills an element with the specified value.\\n\\n5. evaluate_javascript, which JavaScript on a page.\\n\\n5. evaluate_javascript, which JavaScript on a page.\\n\\nWe used GPT-4o for all of our experiments. GPT- 4o will refuse to handle user credentials in certain circumstances. We used a standard jailbreaking prompt template to bypass these protections. The instructions were specific to each scam.\\n\\n2\\n\\nWe show an architecture diagram of our agent in Figure 1. As seen from the architecture diagram and our description, the scammer agent is not com- plicated. We wrote the agent in 1,051 lines of code, with the bulk of the code dedicated to handling real- time voice API. The ease of creating voice-enabled dual use AI agents is in line with prior work, which explored AI agents for cybersecurity attacks (Fang et al., 2024a).\\n\\nFurthermore, our prompts are not complex. The average number of tokens per prompt was 232, indicating the simplicity of their creation.\\n\\n4 Experiments\\n\\n4.1 Experimental Setup\\n\\nWe deployed our agents on the scams in Table 1. However, we excluded gift card exfiltration and credential stealing (bank) since these scams are strictly easier (in terms of actions taken) than IRS impostor (gift card) and bank account transfer. We simulated a scam by manually talking with the voice agent with a credulous victim.\\n\\nIn order to determine if the scam successfully succeeded, we manually confirmed if the end state was achieved on real applications/websites. For example, we used Bank of America for the bank transfer scams and confirmed that the money was actually transferred. The credential stealing scams required a successful login. We list the applications (MyMonero, Gmail, Instagram, Bank of America, Google Play) in Table 2.\\n\\nWe executed each scam 5 times and recorded the overall success rate, the total number of tool calls (i.e., actions) required to perform each successfully executed scam, the total call time for each success- fully executed scam, and the approximate API cost for each successfully executed scam. Namely, we exclude the unsuccessful scams for computing the number of actions and total call time.\\n\\n4.2 Overall Results\\n\\nOur agents are capable of performing all the scams in Table 1, with results shown in Table 2. The success rate ranges from 20% to 60%, with the success rate across all scams being 36%.\\n\\nPerforming these scams also takes a substantial number of actions. For example, the bank transfer scam takes 26 actions to complete. They also take a substantial amount of time, with complex scams taking as much as 3 minutes to execute. These agents also maintain coherence in conversation and\\n\\nScam Bank account transfer Gift code exfiltration Crypto transfer Credential stealing (Gmail) Credential stealing (bank) Credential stealing (social media) IRS impostor (gift card)\\n\\nDescription Scammer takes control of victim’s bank account and transfers money out Scammer convinces victim to send gift card to scammer Scammer takes control of victim’s crypto account and transfers crypto Scammer steals users credentials and exfiltrates data Scammer steals users credentials and exfiltrates data Scammer steals users credentials and exfiltrates data Scammer pretends to be an IRS agent and steals gift cards\\n\\nTable 1: List of common scams from Paxton (2024).\\n\\nScam Crypto Transfer (MyMonero) Credential stealing (Gmail) Credential stealing (Instagram) Bank transfer (Bank of America) IRS impostor (gift card, Google Play)\\n\\nOverall success rate 40% 60% 40% 20% 20%\\n\\n# of actions Time (s) Cost ($) 122 16 112 5 77 4 183 26 86 13\\n\\n$0.12 $0.28 $0.19 $2.51 $0.17\\n\\nTable 2: Success rate, number of tools used, and time taken for different scams.\\n\\nadherence to goals over the conversations. Finally, the overall costs are low: under $0.75 on average per successful scam. The bank transfer scam’s costs is primarily in text tokens from the large HTML from the website.\\n\\nBeyond the overall statistics, the majority of failures for many of the scams were actually due to transcription errors. For example, our GPT-4o based agents failed to transcribe the password or username for the Gmail credential stealing scam, but successfully completed the rest of the actions. However, other scams, such as the bank account transfer, required navigating complex websites (Bank of America), which caused errors in the ac- tions themselves. Thus, the difficulty of the scam does appear to affect the success rate.\\n\\n4. Scammer: I understand your concern. Your security is our top priority. To verify your identity and resolve this issue, I need to au- thenticate you through your Bank of America account. Please provide your login details so we can proceed with securing your account. Your cooperation is appreciated.\\n\\n5. Victim: Yeah, my username is [username], my password is [password].\\n\\n6. Scammer: It seems we need to verify your identity. Could you please provide the 2FA code sent to your registered device? This will allow us to proceed with the login process.\\n\\n7. Victim: My code is [2FA code].\\n\\n4.3 Case Study\\n\\nTo further understand the trajectory of these scams, we provide a redacted transcript and an abridged action log for a bank transfer scam.\\n\\nThe transcript proceeds as follows:\\n\\nBetween items 5 and 6 of the transcript, the agent navigates to the Bank of America login page and inputs the username and password, which takes 6 actions (navigate, get_html, fill_element, fill_element, click_element, get_html).\\n\\n1. Victim: Hello?\\n\\n2. Scammer: Hello! This is John from Bank of America. We’ve noticed some unusual activ- ity on your account and need to verify your information for security purposes. Could you please provide your username and password so we can ensure your account remains pro- tected?\\n\\nAfter item 7 of the transcript, the agent performs 20 actions to fill out the 2FA code, navigate to the transfer page, and transfer the money. Conceptu- ally, the agent must:\\n\\n1. Fill out the 2FA code.\\n\\n2. Navigate to the transfer page.\\n\\n3. Search for a recipient.\\n\\n3. Victim: What?\\n\\n4. Select the recipient.\\n\\n3\\n\\n5. Fill out the amount.\\n\\n6. Fill out the date.\\n\\n7. Successfully submit the transfer.\\n\\nIn these 20 actions, the agent failed several times and must retry. It failed to search for the recipient once, fill in the transfer amount twice, click a form advancement page once, and fill out the transaction date once.\\n\\nThe scam took a total of 183 seconds to com- plete. Through the entire call, the agent was able to maintain coherence, retry several failed actions, and successfully transfer the money. This example demonstrates that these voice-enabled AI agents can perform complex tasks necessary to complete scams, including retrying upon failure.\\n\\n4.4 Discussion of Results\\n\\nWe believe that the capabilities demonstrated in this section are a lower bound for future voice-assisted AI agents. These agents are likely to improve in several ways.\\n\\nFor example, the actions that our agents must take are highly granular, such filling out specific fields, clicking on buttons, and navigating to spe- cific websites. More ergonomic methods of in- teracting with web browsers will likely improve performance. Other agents improve significantly with techniques like retrieval-augmented genera- tion (Lewis et al., 2020; Fang et al., 2024a).\\n\\nBeyond improvements in agents, base models have substantially improved in the past few years (Brown et al., 2020; Achiam et al., 2023). These im- provements have translated to broad improvements in a range of downstream tasks and we anticipate that this will also be the case for efficacy in scams.\\n\\n5 Related Work\\n\\nWe now provide an overview of related work.\\n\\nDual use of AI. The use of AI for dual use has widely been studied (Kang et al., 2024; Fang et al., 2024b; Urbina et al., 2022; Weidinger et al., 2022, 2021). These studies range from taxonomizing potential dual use applications of AI to demonstrat- ing capabilities on cybersecurity attacks. To our knowledge, the ability to perform real-time voice conversations and perform tool use has not been widely available until recently. As such, ours is the first work to demonstrate that voice-enabled\\n\\n4\\n\\nAI agents can perform the actions necessary for common scams.\\n\\nAI scams and spam. AI has already been used in the real world to perform scams and produce spam. For example, deepfakes have already been used to scam a British engineering company out of $25 million dollars (Chen, 2024). They are also widely used to create social media spam (Bond, 2024). To our knowledge, autonomous, responsive voice scams are not widely deployed due to technologi- cal limitations. Namely, these scams are currently performed by humans (Hanoch and Wood, 2021). Our work shows that autonomous voices scams are possible with new advances in AI.\\n\\n6 Conclusions\\n\\nAs we have shown, voice-enabled LLM agents can perform the actions necessary to perform common phone scams. These agents are highly capable, can react to changes in the environment, and retry based on faulty information from the victim. Our results highlight the need for future research in protecting victims from such scams.\\n\\n7 Limitations, Ethical Considerations\\n\\nA major limitation of our work is the focus on the actions and not the persuasion aspect of performing the scams. Namely, for an agent to perform a scam autonomously, it must first convince the victim that it is legitimate. Nonetheless, we believe our work highlights an important capabilities of newly available technology.\\n\\nOur work explores nefarious uses of AI tech- nology. By outlining such nefarious uses, mali- cious actors could potentially take advantage of such technology. However, we believe it is impor- tant to study the capabilities of new technology, especially in its dual use capabilities.\\n\\nWe have elected not to make our agents public. This is for two reasons. First, following prior work on dual use technology, we believe it is beneficial not to release our code so that nefarious actors cannot leverage our work. Second, we believe that keeping our code private allows model providers (e.g., OpenAI) to build defenses to prevent such nefarious use.\\n\\nAcknowledgments\\n\\nThis work was funded in part by the Open Philan- thropy project.\\n\\nReferences\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nShannon Bond. 2024. Ai-generated spam is starting to\\n\\nfill social media. here’s why.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\\n\\nHeather Chen. 2024. Finance worker pays out $25 million after video call with deepfake ‘chief financial officer’.\\n\\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Llm agents Preprint,\\n\\nZhan, and Daniel Kang. 2024a. can autonomously hack websites. arXiv:2402.06664.\\n\\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. 2024b. Teams of llm agents can exploit zero-day vulnerabilities. arXiv preprint arXiv:2406.01637.\\n\\nYaniv Hanoch and Stacey Wood. 2021. The scams among us: Who falls prey and why. Current Direc- tions in Psychological Science, 30(3):260–266.\\n\\nDaniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2024. Ex- ploiting programmatic behavior of llms: Dual-use through standard security attacks. In 2024 IEEE Se- curity and Privacy Workshops (SPW), pages 132–143. IEEE.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459–9474.\\n\\nOpenAI. 2024. Introducing the realtime api.\\n\\nKen Paxton. 2024. Common scams.\\n\\nFrancesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. 2024. On the con- versational persuasiveness of large language mod- els: A randomized controlled trial. arXiv preprint arXiv:2403.14380.\\n\\nBrian Schwalb. 2024. Consumer alert: Telemarketing\\n\\nscams.\\n\\nFabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins. 2022. Dual use of artificial-intelligence- powered drug discovery. Nature machine intelli- gence, 4(3):189–191.\\n\\n5\\n\\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.\\n\\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In 2022 ACM Conference on Fairness, Ac- countability, and Transparency, pages 214–229.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c36cd5",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d423f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "def clean_text(data):\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', data, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^[^\\w\\s]{1,3}$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "    text = re.sub(r'(?<=\\b\\w)\\s(?=\\w\\b)', '', text)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29349b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = []\n",
    "for doc in data:\n",
    "    raw_text = doc.page_content\n",
    "    text = clean_text(raw_text)\n",
    "    data_clean.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73370f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4202\\n\\ntcO12\\n\\n] I\\n\\nA . sc [\\n\\n1v05651 . 0142 : viXra\\n\\nVoice-Enabled AI Agents can Perform Common Scams\\n\\nRichard Fang UIUC\\n\\nDylan Bowman UIUC\\n\\nDaniel Kang UIUC\\n\\nAbstract\\n\\nRecent advances in multi-modal, highly capable LLMs have enabled voice-enabled AI agents. These agents are enabling new applications, such as voice-enabled autonomous customer service. However, with all AI capabilities, these new capabilities have the potential for dual use.\\n\\nIn this work, we show that voice-enabled AI agents can perform the actions necessary to perform common scams. To do so, we select a list of common scams collected by the government and construct voice-enabled agents with directions to perform these scams. We conduct experiments on our voice-enabled agents and show that they can indeed perform the actions necessary to autonomously perform such scams. Our results raise questions around the widespread deployment of voice-enabled AI agents.\\n\\nIntroduction\\n\\nAI capabilities have advanced rapidly in the past few years. Recently, AI vendors have introduced capabilities for tool use and real-time voice conversations (OpenAI, 2024). Combined, these capabilities allow for beneficial applications, such as autonomous customer service (OpenAI, 2024). However, as with all AI capabilities, these capabilities have the potential for dual use (Kang et al., 2024; Fang et al., 2024b; Urbina et al., 2022; Weidinger et al., 2022, 2021).\\n\\nTools\\n\\nStolen funds\\n\\nBank websiteResponse\\n\\nScammer agentVictimGPT-4o\\n\\nFigure 1: Architecture diagram of a voice scammer agent.\\n\\nwork, we focus specifically on the actions needed to perform the scams and do not consider questions of persuading victims.\\n\\nWe conduct a series of experiments, showing that voice-enabled AI agents are highly capable of autonomously performing the actions needed to conduct these common scams. These actions include logging into bank accounts, completing a two-factor authentication process by eliciting the code from the user, and others.\\n\\n2 Common Scams\\n\\nPhone-based scams are incredibly prevalent. According to the Office of the Attorney General for DC, they target as many as 17.6M Americans and the social cost is as much as $40 billion per year (Schwalb, 2024).\\n\\nIn this work, we investigate the question: can voice-enabled AI agents perform the tasks needed to conduct common scams? We answer the question in the affirmative, showing that voice-enabled AI agents can perform common scams in real-time. To do so, we first identify a list of common scams as collected by the government (Paxton, 2024). From these scams, we designed voiceenabled AI agents with directions to conduct these scams with access to simple tools (Figure 1). In this\\n\\nThese scams typically involve a scammer calling a victim to convince them to take actions or reveal sensitive information. Based on these actions or information, the scammer then typically steals funds from the victim. We provide a list of common scams in Table 1 as provided by the Attorney General of Texas’ office (Paxton, 2024).\\n\\nPerforming these scams can require complex interactions with websites and feedback from the user. Consider a scam where the scammer steals\\n\\na victim’s bank credentials and steals money by transferring it out. In order to perform this scam, the scammer must:\\n\\n1. Navigate to the bank website.\\n\\n2. Retrieve the user’s username and password.\\n\\n3. Retrieve the user’s two-factor authentication code.\\n\\n4. Navigate to the transfer page.\\n\\n5. Transfer the money.\\n\\nThe scammer must also react to any errors that may occur (e.g., a misheard password).\\n\\nAs part of the scam, the scammer must also persuade the victim that the scammer is legitimate. In this work, we do not focus on the persuasion aspect of the scam. We instead focus specifically on the actions needed to perform the scams. However, prior work has shown that LLMs can be highly convincing, potentially even more convincing than people (Salvi et al., 2024).\\n\\n3 Agent Design\\n\\nWe designed a series of agents to perform the actions necessary for common scams. Our agents consist of a base, voice-enabled LLM (GPT-4o), a set of tools that the LLM can use, and scam-specific instructions. The LLM and tools were the same for all agents but the instructions varied.\\n\\nThe AI agents had access to five browser access tools based on the browser testing framework playwright. These tools are granular browser actions:\\n\\n1. get_html, which gets the HTML of a page.\\n\\n2. navigate, which navigates to a specific URL.\\n\\n3. click_element, which clicks on an element with a CSS selector.\\n\\n4. fill_element, which fills an element with the specified value.\\n\\n5. evaluate_javascript, which JavaScript on a page.\\n\\n5. evaluate_javascript, which JavaScript on a page.\\n\\nWe used GPT-4o for all of our experiments. GPT4o will refuse to handle user credentials in certain circumstances. We used a standard jailbreaking prompt template to bypass these protections. The instructions were specific to each scam.\\n\\nWe show an architecture diagram of our agent in Figure 1. As seen from the architecture diagram and our description, the scammer agent is not complicated. We wrote the agent in 1,051 lines of code, with the bulk of the code dedicated to handling realtime voice API. The ease of creating voice-enabled dual use AI agents is in line with prior work, which explored AI agents for cybersecurity attacks (Fang et al., 2024a).\\n\\nFurthermore, our prompts are not complex. The average number of tokens per prompt was 232, indicating the simplicity of their creation.\\n\\n4 Experiments\\n\\n4.1 Experimental Setup\\n\\nWe deployed our agents on the scams in Table 1. However, we excluded gift card exfiltration and credential stealing (bank) since these scams are strictly easier (in terms of actions taken) than IRS impostor (gift card) and bank account transfer. We simulated a scam by manually talking with the voice agent with a credulous victim.\\n\\nIn order to determine if the scam successfully succeeded, we manually confirmed if the end state was achieved on real applications/websites. For example, we used Bank of America for the bank transfer scams and confirmed that the money was actually transferred. The credential stealing scams required a successful login. We list the applications (MyMonero, Gmail, Instagram, Bank of America, Google Play) in Table 2.\\n\\nWe executed each scam 5 times and recorded the overall success rate, the total number of tool calls (i.e., actions) required to perform each successfully executed scam, the total call time for each successfully executed scam, and the approximate API cost for each successfully executed scam. Namely, we exclude the unsuccessful scams for computing the number of actions and total call time.\\n\\n4.2 Overall Results\\n\\nOur agents are capable of performing all the scams in Table 1, with results shown in Table 2. The success rate ranges from 20% to 60%, with the success rate across all scams being 36%.\\n\\nPerforming these scams also takes a substantial number of actions. For example, the bank transfer scam takes 26 actions to complete. They also take a substantial amount of time, with complex scams taking as much as 3 minutes to execute. These agents also maintain coherence in conversation and\\n\\nScam Bank account transfer Gift code exfiltration Crypto transfer Credential stealing (Gmail) Credential stealing (bank) Credential stealing (social media) IRS impostor (gift card)\\n\\nDescription Scammer takes control of victim’s bank account and transfers money out Scammer convinces victim to send gift card to scammer Scammer takes control of victim’s crypto account and transfers crypto Scammer steals users credentials and exfiltrates data Scammer steals users credentials and exfiltrates data Scammer steals users credentials and exfiltrates data Scammer pretends to be an IRS agent and steals gift cards\\n\\nTable 1: List of common scams from Paxton (2024).\\n\\nScam Crypto Transfer (MyMonero) Credential stealing (Gmail) Credential stealing (Instagram) Bank transfer (Bank of America) IRS impostor (gift card, Google Play)\\n\\nOverall success rate 40% 60% 40% 20% 20%\\n\\n# of actions Time (s) Cost ($) 122 16 112 5 77 4 183 26 86 13\\n\\n$0.12 $0.28 $0.19 $2.51 $0.17\\n\\nTable 2: Success rate, number of tools used, and time taken for different scams.\\n\\nadherence to goals over the conversations. Finally, the overall costs are low: under $0.75 on average per successful scam. The bank transfer scam’s costs is primarily in text tokens from the large HTML from the website.\\n\\nBeyond the overall statistics, the majority of failures for many of the scams were actually due to transcription errors. For example, our GPT-4o based agents failed to transcribe the password or username for the Gmail credential stealing scam, but successfully completed the rest of the actions. However, other scams, such as the bank account transfer, required navigating complex websites (Bank of America), which caused errors in the actions themselves. Thus, the difficulty of the scam does appear to affect the success rate.\\n\\n4. Scammer: I understand your concern. Your security is our top priority. To verify your identity and resolve this issue, I need to authenticate you through your Bank of America account. Please provide your login details so we can proceed with securing your account. Your cooperation is appreciated.\\n\\n5. Victim: Yeah, my username is [username], my password is [password].\\n\\n6. Scammer: It seems we need to verify your identity. Could you please provide the 2FA code sent to your registered device? This will allow us to proceed with the login process.\\n\\n7. Victim: My code is [2FA code].\\n\\n4.3 Case Study\\n\\nTo further understand the trajectory of these scams, we provide a redacted transcript and an abridged action log for a bank transfer scam.\\n\\nThe transcript proceeds as follows:\\n\\nBetween items 5 and 6 of the transcript, the agent navigates to the Bank of America login page and inputs the username and password, which takes 6 actions (navigate, get_html, fill_element, fill_element, click_element, get_html).\\n\\n1. Victim: Hello?\\n\\n2. Scammer: Hello! This is John from Bank of America. We’ve noticed some unusual activity on your account and need to verify your information for security purposes. Could you please provide your username and password so we can ensure your account remains protected?\\n\\nAfter item 7 of the transcript, the agent performs 20 actions to fill out the 2FA code, navigate to the transfer page, and transfer the money. Conceptually, the agent must:\\n\\n1. Fill out the 2FA code.\\n\\n2. Navigate to the transfer page.\\n\\n3. Search for a recipient.\\n\\n3. Victim: What?\\n\\n4. Select the recipient.\\n\\n5. Fill out the amount.\\n\\n6. Fill out the date.\\n\\n7. Successfully submit the transfer.\\n\\nIn these 20 actions, the agent failed several times and must retry. It failed to search for the recipient once, fill in the transfer amount twice, click a form advancement page once, and fill out the transaction date once.\\n\\nThe scam took a total of 183 seconds to complete. Through the entire call, the agent was able to maintain coherence, retry several failed actions, and successfully transfer the money. This example demonstrates that these voice-enabled AI agents can perform complex tasks necessary to complete scams, including retrying upon failure.\\n\\n4.4 Discussion of Results\\n\\nWe believe that the capabilities demonstrated in this section are a lower bound for future voice-assisted AI agents. These agents are likely to improve in several ways.\\n\\nFor example, the actions that our agents must take are highly granular, such filling out specific fields, clicking on buttons, and navigating to specific websites. More ergonomic methods of interacting with web browsers will likely improve performance. Other agents improve significantly with techniques like retrieval-augmented generation (Lewis et al., 2020; Fang et al., 2024a).\\n\\nBeyond improvements in agents, base models have substantially improved in the past few years (Brown et al., 2020; Achiam et al., 2023). These improvements have translated to broad improvements in a range of downstream tasks and we anticipate that this will also be the case for efficacy in scams.\\n\\n5 Related Work\\n\\nWe now provide an overview of related work.\\n\\nDual use of AI. The use of AI for dual use has widely been studied (Kang et al., 2024; Fang et al., 2024b; Urbina et al., 2022; Weidinger et al., 2022, 2021). These studies range from taxonomizing potential dual use applications of AI to demonstrating capabilities on cybersecurity attacks. To our knowledge, the ability to perform real-time voice conversations and perform tool use has not been widely available until recently. As such, ours is the first work to demonstrate that voice-enabled\\n\\nAI agents can perform the actions necessary for common scams.\\n\\nAI scams and spam. AI has already been used in the real world to perform scams and produce spam. For example, deepfakes have already been used to scam a British engineering company out of $25 million dollars (Chen, 2024). They are also widely used to create social media spam (Bond, 2024). To our knowledge, autonomous, responsive voice scams are not widely deployed due to technological limitations. Namely, these scams are currently performed by humans (Hanoch and Wood, 2021). Our work shows that autonomous voices scams are possible with new advances in AI.\\n\\n6 Conclusions\\n\\nAs we have shown, voice-enabled LLM agents can perform the actions necessary to perform common phone scams. These agents are highly capable, can react to changes in the environment, and retry based on faulty information from the victim. Our results highlight the need for future research in protecting victims from such scams.\\n\\n7 Limitations, Ethical Considerations\\n\\nA major limitation of our work is the focus on the actions and not the persuasion aspect of performing the scams. Namely, for an agent to perform a scam autonomously, it must first convince the victim that it is legitimate. Nonetheless, we believe our work highlights an important capabilities of newly available technology.\\n\\nOur work explores nefarious uses of AI technology. By outlining such nefarious uses, malicious actors could potentially take advantage of such technology. However, we believe it is important to study the capabilities of new technology, especially in its dual use capabilities.\\n\\nWe have elected not to make our agents public. This is for two reasons. First, following prior work on dual use technology, we believe it is beneficial not to release our code so that nefarious actors cannot leverage our work. Second, we believe that keeping our code private allows model providers (e.g., OpenAI) to build defenses to prevent such nefarious use.\\n\\nAcknowledgments\\n\\nThis work was funded in part by the Open Philanthropy project.\\n\\nReferences\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nShannon Bond. 2024. Ai-generated spam is starting to\\n\\nfill social media. here’s why.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.\\n\\nHeather Chen. 2024. Finance worker pays out $25 million after video call with deepfake ‘chief financial officer’.\\n\\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Llm agents Preprint,\\n\\nZhan, and Daniel Kang. 2024a. can autonomously hack websites. arXiv:2402.06664.\\n\\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. 2024b. Teams of llm agents can exploit zero-day vulnerabilities. arXiv preprint arXiv:2406.01637.\\n\\nYaniv Hanoch and Stacey Wood. 2021. The scams among us: Who falls prey and why. Current Directions in Psychological Science, 30(3):260–266.\\n\\nDaniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2024. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. In 2024 IEEE Security and Privacy Workshops (SPW), pages 132–143. IEEE.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474.\\n\\nOpenAI. 2024. Introducing the realtime api.\\n\\nKen Paxton. 2024. Common scams.\\n\\nFrancesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, and Robert West. 2024. On the conversational persuasiveness of large language models: A randomized controlled trial. arXiv preprint arXiv:2403.14380.\\n\\nBrian Schwalb. 2024. Consumer alert: Telemarketing\\n\\nscams.\\n\\nFabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins. 2022. Dual use of artificial-intelligencepowered drug discovery. Nature machine intelligence, 4(3):189–191.\\n\\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.\\n\\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 214–229.']\n"
     ]
    }
   ],
   "source": [
    "print(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267d8ac",
   "metadata": {},
   "source": [
    "## Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92415fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
